{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136da7ea",
   "metadata": {},
   "source": [
    "\n",
    "- Step 3: Define the Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "- Step 4: Run ETL to Model the Data\n",
    "Create the data pipelines and the data model\n",
    "Include a data dictionary\n",
    "Run data quality checks to ensure the pipeline ran as expected\n",
    "Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "Unit tests for the scripts to ensure they are doing the right thing\n",
    "Source/count checks to ensure completeness\n",
    "- Step 5: Complete Project Write Up\n",
    "What's the goal? What queries will you want to run? How would Spark or Airflow be incorporated? Why did you choose the model you chose?\n",
    "Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "Document the steps of the process.\n",
    "Propose how often the data should be updated and why.\n",
    "Post your write-up and final data model in a GitHub repo.\n",
    "Include a description of how you would approach the problem differently under the following scenarios:\n",
    "If the data was increased by 100x.\n",
    "If the pipelines were run on a daily basis by 7am.\n",
    "If the database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41262bc",
   "metadata": {},
   "source": [
    "# The aim of this project\n",
    "\n",
    "The aim of this project is to reverse engineer a private API and use the data gained from this exercise to create a DWH on Redshift. I will bring the API data to a normalized form, and populate the purchases table with dummy data generated from faker, based on the already existing tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9242fd",
   "metadata": {},
   "source": [
    "# Reverse-engineering the API\n",
    "\n",
    "The only place I was able to find a comprehensive dataset on gins is stuck behind an iOS/Android app's private API. The following is a high level description of how I accessed the base URL, API structure and API key.\n",
    "- The simplest way to approach the problem is to use an Android emulator, in this case I used Android Studio. Another option would be to use mitmproxy, but due to Android's strict Certificate Authority management it is a bit finicky to setup with a system certificate on an Android emulator.\n",
    "- I downloaded the target app APK, and installed it on the emulted device.\n",
    "- Installed ADB, and made sure to add the platform-tools folder to your PATH variable.\n",
    "- Installed HTTP Toolkit. Selected Android device via ADB as my traffic source and followed setup steps in the emulator.\n",
    "- Done! Now I can see HTTP requests coming in from the emulator.\n",
    "- All there is left to do is find the GET request that I am after, I have done this by clicking on a new item on the main page of the application. The resulting API key and structure is what I use in my code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6272d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from requests.exceptions import HTTPError\n",
    "import sql_statements\n",
    "import configparser\n",
    "\n",
    "# Variables\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "baseURL = config.get(\"REQUESTS\", \"baseURL\")\n",
    "api_key = config.get(\"REQUESTS\", \"api_key\")\n",
    "headers = {\n",
    "    'User-Agent': config.get(\"REQUESTS\", \"headers_user_agent\"),\n",
    "    'From': config.get(\"REQUESTS\", \"headers_from\")\n",
    "}\n",
    "\n",
    "raw_folder = config.get(\"FOLDER\", \"raw_folder\")\n",
    "normalized_folder = config.get(\"FOLDER\", \"normalized_folder\")\n",
    "\n",
    "#Identifiers & Credentials\n",
    "DB_NAME=config.get(\"CLUSTER\", \"DB_NAME\")\n",
    "CLUSTER_ID=config.get(\"CLUSTER\", \"CLUSTER_ID\")\n",
    "DB_USER=config.get(\"CLUSTER\", \"DB_USER\")\n",
    "DB_PASSWORD=config.get(\"CLUSTER\", \"DB_PASSWORD\")\n",
    "CLUSTER_TYPE=config.get(\"CLUSTER\", \"CLUSTER_TYPE\")\n",
    "NODE_TYPE=config.get(\"CLUSTER\", \"NODE_TYPE\")\n",
    "NUMBER_OF_NODES=config.get(\"CLUSTER\", \"NUMBER_OF_NODES\")\n",
    "DB_PORT=config.get(\"CLUSTER\", \"DB_PORT\")\n",
    "\n",
    "ROLE_NAME = config.get(\"IAM_ROLE\", \"ROLE_NAME\")\n",
    "KEY = config.get(\"AWS\", \"KEY\")\n",
    "SECRET = config.get(\"AWS\", \"SECRET\")\n",
    "REGION = config.get(\"AWS\", \"REGION\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\",\n",
    "                       region_name=REGION,\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=REGION,\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "iam = boto3.client('iam',\n",
    "                       region_name=REGION,\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=REGION,\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "redshift_data = boto3.client('redshift-data',\n",
    "                       region_name=REGION,\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5a02e",
   "metadata": {},
   "source": [
    "# Pull the data from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad80ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest ID number from ginventory_short.json\n",
    "# A version of the dataset is available from one of the first calls the app makes when establishing connection,\n",
    "# with a reduced set of columns. The largest id num (8770) comes from this file (ginventory_short.json).\n",
    "with open('/'.join([raw_folder, 'ginventory_short.json']), 'r', encoding='utf-8') as file:\n",
    "    file_json = json.load(file)\n",
    "\n",
    "df = pd.DataFrame(file_json)\n",
    "df['id'] = df['id'].apply(pd.to_numeric)\n",
    "largest_id = df.sort_values('id', ascending=False).head(1)\n",
    "largest_id = largest_id.iloc[0,0]\n",
    "print(largest_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b74bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looping through all the requests, save responses to local and S3\n",
    "response_collection = []\n",
    "for i in range(1,largest_id+1):\n",
    "    url = baseURL.format(i, api_key)\n",
    "    try:\n",
    "        response = requests.get(url, headers = headers)\n",
    "        response.raise_for_status()\n",
    "        print(i)\n",
    "        response_collection.append(response.json())\n",
    "        time.sleep(0.1)\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "    else:\n",
    "        print('Success!')\n",
    "    \n",
    "with open('/'.join([raw_folder, 'data_full.json']), 'w', encoding='utf-8') as f:\n",
    "    json.dump(response_collection, f, ensure_ascii=False, indent=4)\n",
    "print(\"Responses collected!\")\n",
    "\n",
    "# Upload raw file to s3\n",
    "s3_client.upload_file('/'.join([raw_folder, 'data_full.json']), \"ginventory-bucket\", '/'.join([raw_folder, 'data_full.json']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2971fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read from local file\n",
    "with open('/'.join([raw_folder, 'data_full.json']), 'r', encoding='utf-8') as file:\n",
    "    file_json = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324f10d",
   "metadata": {},
   "source": [
    "# Data quality issues\n",
    "\n",
    "- Country and abv had cases where 2 values were included, seperated by /.\n",
    "- Columns that should be integer have been cast as such.\n",
    "- Country names did not always match ISO standards, in such cases the country name was replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03099878",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571ebe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(file_json)\n",
    "df = df.drop(['direct_purchase_url', 'user_rating', 'in_wishlist', 'in_cabinet', 'purchase_links.data',\n",
    "              'perfect_tonics.data','perfect_garnishes.data', 'perfect_gins.data', 'description.content', \n",
    "              'description.google_translation', 'description.original_content'], axis=1)\n",
    "\n",
    "\n",
    "# Correct abv values where format is ##/##, first number is taken\n",
    "#df['abv'] = df[df[\"abv\"].str.contains(\"/\", na=False)].abv.str.split('/').str.get(0).str.strip()\n",
    "df['abv'] = np.where(df[\"abv\"].str.contains(\"/\", na=False), df['abv'].str.split('/').str.get(0).str.strip(), df['abv'])\n",
    "\n",
    "# Correct values with % in them, change commas to dots\n",
    "df['abv'] = df['abv'].replace('%','', regex=True).replace(',','.', regex=True)\n",
    "\n",
    "# Handling numeric columns to adhere to correct data types in downstream pipeline\n",
    "df['abv'] = pd.to_numeric(arg=df['abv'] ,errors='coerce')\n",
    "df['average_rating'] = df['average_rating'].apply(pd.to_numeric)\n",
    "\n",
    "# Fill NA with 0, so we can downcast all values to integer\n",
    "df['rating_count'] = df['rating_count'].fillna(0)\n",
    "df['rating_count'] = pd.to_numeric(arg=df['rating_count'],downcast='integer')\n",
    "\n",
    "# Correcting country column.\n",
    "# Handling the case where country format = country / country (eg.: Switzerland / United States)\n",
    "#df['country'] = df[df['country'].str.contains('/', na=False)].country.str.split('/').str.get(0).str.strip()\n",
    "df['country'] = np.where(df['country'].str.contains('/', na=False), df['country'].str.split('/').str.get(0).str.strip(), df['country'])\n",
    "\n",
    "# Country names that do not adhere to ISO country naming standards are replaced\n",
    "country_corrections = {\n",
    "    'Vietnam': 'Viet Nam',\n",
    "    'Russia' : 'Russian Federation',\n",
    "    'Taiwan' : 'Taiwan, Province of China',\n",
    "    'U.S. Virgin Islands' : 'Virgin Islands, U.s.',\n",
    "    'Hong Kong SAR China' : 'Hong Kong',\n",
    "    'Unknown or Invalid Region' : None,\n",
    "    '' : None,\n",
    "    }\n",
    "\n",
    "for i in country_corrections:\n",
    "    df['country'] = np.where(df.country == i, country_corrections[i], df['country'])\n",
    "\n",
    "df['country'] = np.where(df.producer == 'Little Brown Dog Spirits', 'United Kingdom', df['country'])\n",
    "\n",
    "df_garnish = df[df.type == 'garnish']\n",
    "df_garnish = df_garnish[['id', 'type', 'name']]\n",
    "\n",
    "# We drop the remaining rows without valid country values.\n",
    "df = df[df.country.isna()==False]\n",
    "\n",
    "df_gin = df[df.type == 'gin']\n",
    "df_tonic = df[df.type == 'tonic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65f6e9",
   "metadata": {},
   "source": [
    "## Save normalized tables to local and S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08f1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(normalized_folder, exist_ok=True)\n",
    "df_gin.to_csv('/'.join([normalized_folder, 'gins.csv']), sep=';', index=False)\n",
    "df_garnish.to_csv('/'.join([normalized_folder, 'garnishes.csv']), sep=';', index=False)\n",
    "df_tonic.to_csv('/'.join([normalized_folder, 'tonics.csv']), sep=';', index=False)\n",
    "\n",
    "s3_client.upload_file('/'.join([normalized_folder, 'gins.csv']), \"ginventory-bucket\", '/'.join([normalized_folder, 'gins.csv']))\n",
    "s3_client.upload_file('/'.join([normalized_folder, 'garnishes.csv']), \"ginventory-bucket\", '/'.join([normalized_folder, 'garnishes.csv']))\n",
    "s3_client.upload_file('/'.join([normalized_folder, 'tonics.csv']), \"ginventory-bucket\", '/'.join([normalized_folder, 'tonics.csv']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe06ac2",
   "metadata": {},
   "source": [
    "## Extract relationships between gins, garnishes and tonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e252a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These tables will form our many-to-many translation tables in our model\n",
    "\n",
    "df = pd.DataFrame(file_json)\n",
    "df = df[df.type == 'gin']\n",
    "\n",
    "# Prepare dataframe with the gin->perfect tonic relationship\n",
    "df[\"perfect_tonics\"] = df[\"perfect_tonics\"].str[\"data\"]\n",
    "df_perfect_tonics = df.explode(\"perfect_tonics\")\n",
    "df_perfect_tonics = pd.concat(\n",
    "    [\n",
    "        df_perfect_tonics,\n",
    "        df_perfect_tonics.pop(\"perfect_tonics\").apply(pd.Series).add_prefix(\"perfect_tonics_\"),\n",
    "    ], axis=1)\n",
    "\n",
    "# Select the necessary columns, drop NA, change data types\n",
    "df_perfect_tonics = df_perfect_tonics[['id', 'perfect_tonics_id']]\n",
    "df_perfect_tonics.dropna(inplace = True)\n",
    "df_perfect_tonics = df_perfect_tonics.astype(int)\n",
    "\n",
    "# Save\n",
    "df_perfect_tonics.to_csv('/'.join([normalized_folder, 'perfect_tonics.csv']), sep=';', index=False)\n",
    "s3_client.upload_file('/'.join([normalized_folder, 'perfect_tonics.csv']), \"ginventory-bucket\", '/'.join([normalized_folder, 'perfect_tonics.csv']))\n",
    "\n",
    "# Prepare dataframe with the gin->perfect garnish relationship\n",
    "df[\"perfect_garnishes\"] = df[\"perfect_garnishes\"].str[\"data\"]\n",
    "df_perfect_garnishes = df.explode(\"perfect_garnishes\")\n",
    "df_perfect_garnishes = pd.concat(\n",
    "    [\n",
    "        df_perfect_garnishes,\n",
    "        df_perfect_garnishes.pop(\"perfect_garnishes\")\n",
    "        .apply(pd.Series)\n",
    "        .add_prefix(\"perfect_garnishes_\"),\n",
    "    ], axis=1)\n",
    "\n",
    "# Select the necessary columns, drop NA, change data types\n",
    "df_perfect_garnishes = df_perfect_garnishes[['id', 'perfect_garnishes_id']]\n",
    "df_perfect_garnishes.dropna(inplace = True)\n",
    "df_perfect_garnishes = df_perfect_garnishes.astype(int)\n",
    "\n",
    "# Save\n",
    "df_perfect_garnishes.to_csv('/'.join([normalized_folder, 'perfect_garnishes.csv']), sep=';', index=False)\n",
    "s3_client.upload_file('/'.join([normalized_folder, 'perfect_garnishes.csv']), \"ginventory-bucket\", '/'.join([normalized_folder, 'perfect_garnishes.csv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07543c29",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Exploring our main dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499a688",
   "metadata": {},
   "source": [
    "##  General counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4131877",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of gins in our dataset: {df_gin.id.count()}')\n",
    "print(f'Total number of garnishes in our dataset: {df_garnish.id.count()}')\n",
    "print(f'Total number of tonics in our dataset: {df_tonic.id.count()}')\n",
    "\n",
    "print(f'{df_perfect_garnishes.nunique().id} Gins have at least one garnish indicated as a perfect match, {df_perfect_garnishes.perfect_garnishes_id.count()} perfect matches in total.')\n",
    "print(f'{df_perfect_tonics.nunique().id} Gins have at least one tonic indicated as a perfect match, {df_perfect_tonics.perfect_tonics_id.count()} perfect matches in total.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best rated gins\n",
    "df_gin_explore = df_gin[[\"name\", \"average_rating\"]]\n",
    "df_gin_explore.sort_values(\"average_rating\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52284e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gin avergae rating histogram\n",
    "df_gin.hist(column='average_rating' ,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e83c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most rated gins \n",
    "df_gin_explore = df_gin[[\"name\", \"rating_count\"]]\n",
    "df_gin_explore.sort_values(\"rating_count\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd22ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gin ratings count histogram\n",
    "# Most gins have 0-20 ratings\n",
    "df_gin.hist(column='rating_count' ,bins=100, range=[0, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best rated producers\n",
    "df_gin_explore = df_gin[[\"producer\", \"average_rating\"]]\n",
    "df_gin_explore.groupby('producer').mean().sort_values(\"average_rating\",ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producer average ratings histogram\n",
    "df_gin.groupby('producer').mean().hist(column='average_rating' ,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries with the most gins\n",
    "df_gin.groupby('country').nunique().id.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries with the most gin producers\n",
    "df_gin.groupby(['country']).nunique().producer.sort_values(ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a740c1",
   "metadata": {},
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452d0640",
   "metadata": {},
   "source": [
    "### IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a75ef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name DWH_ROLE already exists.\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::752709659342:role/DWH_ROLE\n"
     ]
    }
   ],
   "source": [
    "# Create IAM role so we can access S3 from Redshift\n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Attach S3 access policy to our new role\n",
    "\n",
    "iam.attach_role_policy(RoleName=ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b593e6ed",
   "metadata": {},
   "source": [
    "### Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a51de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (ClusterAlreadyExists) when calling the CreateCluster operation: Cluster already exists\n"
     ]
    }
   ],
   "source": [
    "# Create redshift cluster\n",
    "try:\n",
    "    response = redshift.create_cluster(\n",
    "        ClusterType= CLUSTER_TYPE,\n",
    "        NodeType=NODE_TYPE,\n",
    "        NumberOfNodes=int(NUMBER_OF_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DB_NAME,\n",
    "        ClusterIdentifier=CLUSTER_ID,\n",
    "        MasterUsername=DB_USER,\n",
    "        MasterUserPassword=DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9a2c003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>ginventorydwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>ginventory_auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'ginventorydwh.czo11jameqnn.eu-cen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-01c2590b227c9dbaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NumberOfNodes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Key                                              Value\n",
       "0  ClusterIdentifier                                      ginventorydwh\n",
       "1           NodeType                                          dc2.large\n",
       "2      ClusterStatus                                          available\n",
       "3     MasterUsername                                            dwhuser\n",
       "4             DBName                                    ginventory_auto\n",
       "5           Endpoint  {'Address': 'ginventorydwh.czo11jameqnn.eu-cen...\n",
       "6              VpcId                              vpc-01c2590b227c9dbaf\n",
       "7      NumberOfNodes                                                  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prettify redshift cluster properties\n",
    "def prettyRedshiftProps(props):\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=CLUSTER_ID)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20414412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg-0d5e137166258a0e9\n",
      "Rule requested already exists\n"
     ]
    }
   ],
   "source": [
    "# Set a VPC security group rule to authorize ingress to the cluster's VPC Security Group\n",
    "# Extract cluster info\n",
    "\n",
    "vpc_security_group_id = myClusterProps[\"VpcSecurityGroups\"][0][\"VpcSecurityGroupId\"]\n",
    "print(vpc_security_group_id)\n",
    "try:\n",
    "    # Extract security group for the VPC\n",
    "    vpc_sg = ec2.SecurityGroup(id = vpc_security_group_id)\n",
    "    \n",
    "    # Authorize connection to the VPC\n",
    "    vpc_sg.authorize_ingress(\n",
    "        GroupName = vpc_sg.group_name,\n",
    "        CidrIp = \"0.0.0.0/0\",\n",
    "        IpProtocol = \"TCP\",\n",
    "        FromPort = 5439,\n",
    "        ToPort = 5439\n",
    "    )\n",
    "    print(\"Ingress to the VPC authorized\")\n",
    "    \n",
    "except Exception as e:\n",
    "    \n",
    "    # Check if the error is a duplication error\n",
    "    if \"InvalidPermission.Duplicate\" in str(e):\n",
    "        print(\"Rule requested already exists\")\n",
    "    else:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d49e2f7",
   "metadata": {},
   "source": [
    "# Load data to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d0f716e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the BatchExecuteStatement operation: Redshift endpoint doesn't exist in this region.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10624/3632230317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Drop and create all tables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m response = redshift_data.batch_execute_statement(\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mClusterIdentifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ginventoryDWH\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mDatabase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ginventory_auto\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\letso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m                 )\n\u001b[0;32m    507\u001b[0m             \u001b[1;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\letso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\botocore\\client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the BatchExecuteStatement operation: Redshift endpoint doesn't exist in this region."
     ]
    }
   ],
   "source": [
    "# Drop and create all tables\n",
    "\n",
    "response = redshift_data.batch_execute_statement(\n",
    "        ClusterIdentifier=\"ginventoryDWH\",\n",
    "        Database=\"ginventory_auto\",\n",
    "        DbUser=\"dwhuser\",\n",
    "        Sqls=[\"SELECT * FROM gins\"]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f7dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tables from S3\n",
    "\n",
    "file_names = ['garnishes', 'gins', 'perfect_garnishes', 'perfect_tonics', 'tonics']\n",
    "\n",
    "sqls = [f\"\"\"\n",
    "        COPY {name}\n",
    "        FROM 's3://ginventory-bucket/normalized_data/{name}.csv'\n",
    "        IAM_ROLE '{roleArn}' \n",
    "        FORMAT AS csv\n",
    "        IGNOREHEADER 1\n",
    "        delimiter ';'\n",
    "    \"\"\" for name in file_names]\n",
    "\n",
    "response = redshift_data.batch_execute_statement(\n",
    "        ClusterIdentifier=CLUSTER_ID,\n",
    "        Database=DB_NAME,\n",
    "        DbUser=DB_USER,\n",
    "        Sqls=sqls\n",
    "    )\n",
    "\n",
    "# Load countries data from SQL inserts\n",
    "response = redshift_data.execute_statement(\n",
    "        ClusterIdentifier=CLUSTER_ID,\n",
    "        Database=DB_NAME,\n",
    "        DbUser=DB_USER,\n",
    "        Sql=sql_statements.INSERT_INTO_COUNTRIES\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return pandas df from redshift response Id\n",
    "\n",
    "def redshift_get_statement_result_to_dataframe(response_id):\n",
    "    '''\n",
    "    Returns pandas dataframe of the statement Id passed from the response object \n",
    "    of the redshift-data.execute_statement method. If statement has no result set\n",
    "    a message is returned.\n",
    "    response = redshift_data.execute_statement(...)\n",
    "    response_id = response['Id]\n",
    "    '''\n",
    "    # Timeout after 30 seconds of no FAILED or FINISHED status response\n",
    "    timeout = time.time() + 30\n",
    "\n",
    "    while time.time() < timeout:\n",
    "        describe_obj = redshift_data.describe_statement(Id=response_id)\n",
    "        status = describe_obj['Status']\n",
    "        if status == 'FINISHED':\n",
    "            if describe_obj['HasResultSet'] == True:\n",
    "                statement_result = redshift_data.get_statement_result(Id=response_id)\n",
    "                try:\n",
    "                    if len(statement_result['Records']) == 0:\n",
    "                        return 'Result set contains 0 rows.'\n",
    "                    df = pd.DataFrame(statement_result['Records'])\n",
    "                    df.rename(columns=pd.DataFrame(statement_result['ColumnMetadata']).name, inplace=True)\n",
    "                    df = df.apply(pd.Series)\n",
    "\n",
    "                    for column in df.columns:\n",
    "                        df[column] = df[column].apply(pd.Series)\n",
    "                    return df\n",
    "                except Exception as e:\n",
    "                    raise ValueError('Query status: ' + status + '\\n' + describe_obj['Error'])\n",
    "            else:\n",
    "                return 'Statement has no result set. Call with sql statement that produces a result set.'\n",
    "        elif status == 'FAILED':\n",
    "            raise ValueError('Query status: ' + status + '\\n' + describe_obj['Error'])\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        raise TimeoutError('Timeout limit exceeded.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be852c8",
   "metadata": {},
   "source": [
    "# Data checks, further data modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af46cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all country names adhere to the country table.\n",
    "# We expect to see 0 rows returned\n",
    "\n",
    "sql_statement = '''\n",
    "    SELECT * FROM\n",
    "    (\n",
    "        SELECT * FROM gins \n",
    "        LEFT JOIN countries \n",
    "        ON gins.country = countries.nicename \n",
    "        WHERE countries.id is Null\n",
    "    ) \n",
    "        AS country_mismatches;\n",
    "'''\n",
    "\n",
    "response = redshift_data.execute_statement(\n",
    "        ClusterIdentifier=CLUSTER_ID,\n",
    "        Database=DB_NAME,\n",
    "        DbUser=DB_USER,\n",
    "        Sql=sql_statement\n",
    "    )\n",
    "\n",
    "\n",
    "redshift_get_statement_result_to_dataframe(response['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add country_id column to tonics and gins tables, delete country column\n",
    "\n",
    "sqls = []\n",
    "\n",
    "sqls.append('''\n",
    "    ALTER TABLE tonics\n",
    "    ADD country_id smallint;\n",
    "''')\n",
    "sqls.append('''\n",
    "    UPDATE tonics SET country_id = countries.id FROM countries JOIN tonics t on t.country=countries.nicename\n",
    "''')\n",
    "sqls.append('''\n",
    "    ALTER TABLE tonics DROP country\n",
    "''')\n",
    "\n",
    "sqls.append('''\n",
    "    ALTER TABLE gins\n",
    "    ADD country_id smallint;\n",
    "''')\n",
    "sqls.append('''\n",
    "    UPDATE gins SET country_id = countries.id FROM countries JOIN gins g on g.country=countries.nicename\n",
    "''')\n",
    "sqls.append('''\n",
    "    ALTER TABLE gins DROP country\n",
    "''')\n",
    "\n",
    "response = redshift_data.batch_execute_statement(\n",
    "    ClusterIdentifier=CLUSTER_ID,\n",
    "    Database=DB_NAME,\n",
    "    DbUser=DB_USER,\n",
    "    Sqls=sqls,\n",
    "    )\n",
    "\n",
    "redshift_get_statement_result_to_dataframe(response['Id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8947ee",
   "metadata": {},
   "source": [
    "# Delete Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16760427",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift.delete_cluster( ClusterIdentifier=CLUSTER_ID,  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=CLUSTER_ID)['Clusters'][0]\n",
    "print(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58ea52",
   "metadata": {},
   "source": [
    "# Delete role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf7e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(RoleName=ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94b3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "548ef4a45503cf6cb57d5f5808a981105b25d5d40770224cf160d37fce6cb67e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
